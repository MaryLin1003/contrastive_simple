
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import time
import os
import numpy as np

# å¯¼å…¥æ¨¡å‹åˆ›å»ºå‡½æ•°
from .models import create_model

class BaseTrainer:
    """åŸºç¡€è®­ç»ƒå™¨ - æ”¯æŒç›‘ç£å­¦ä¹ ã€SimCLRã€MoCo"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.optimizer = None
        self.criterion = nn.CrossEntropyLoss()  # ç»Ÿä¸€ä½¿ç”¨äº¤å‰ç†µæŸå¤±
        
    def setup(self):
        """è®¾ç½®æ¨¡å‹ã€ä¼˜åŒ–å™¨"""
        model_name = self.config['model']['name']
        self.model = create_model(model_name, self.config['model']).to(self.device)
        
        # ä¼˜åŒ–å™¨
        lr = self.config['training']['learning_rate']
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=self.config['training'].get('weight_decay', 1e-4)
        )
        
        return self.model
    
    def train_epoch(self, train_loader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒè®­ç»ƒé€»è¾‘"""
        self.model.train()
        total_loss = 0
        total_acc = 0
        total_samples = 0
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
        
        for batch_idx, batch in enumerate(pbar):
            model_name = self.config['model']['name']
            
            if model_name == 'supervised':
                loss, acc, batch_size = self._train_supervised_batch(batch)
                
            elif model_name == 'simclr':
                loss, acc, batch_size = self._train_simclr_batch(batch)
                
            elif model_name == 'moco':
                loss, acc, batch_size = self._train_moco_batch(batch)
                
            else:
                raise ValueError(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_name}")
            
            total_loss += loss * batch_size
            total_acc += acc * batch_size
            total_samples += batch_size
            
            pbar.set_postfix({
                'loss': f'{loss:.4f}',
                'acc': f'{acc:.2%}'
            })
        
        avg_loss = total_loss / total_samples
        avg_acc = total_acc / total_samples
        return avg_loss, avg_acc
    
    def _train_supervised_batch(self, batch):
        """ç›‘ç£å­¦ä¹ å•æ‰¹æ¬¡è®­ç»ƒ"""
        x1, x2, labels = batch
        x1, labels = x1.to(self.device), labels.to(self.device)
        
        self.optimizer.zero_grad()
        logits, _ = self.model(x1)
        loss = self.criterion(logits, labels)
        
        # è®¡ç®—å‡†ç¡®ç‡
        _, predicted = logits.max(1)
        acc = (predicted == labels).float().mean().item()
        
        loss.backward()
        self.optimizer.step()
        
        return loss.item(), acc, x1.size(0)
    
    def _train_simclr_batch(self, batch):
        """SimCLRå•æ‰¹æ¬¡è®­ç»ƒ - ä¿®æ­£ç‰ˆæŸå¤±å‡½æ•°"""
        x1, x2, _ = batch
        x1, x2 = x1.to(self.device), x2.to(self.device)
        
        self.optimizer.zero_grad()
        z1, z2, _, _ = self.model(x1, x2)
        
        batch_size = z1.shape[0]
        
        # SimCLRæŸå¤±å®ç°
        # 1. L2å½’ä¸€åŒ–
        z1 = nn.functional.normalize(z1, dim=1)
        z2 = nn.functional.normalize(z2, dim=1)
        
        # 2. æ‹¼æ¥ç‰¹å¾
        features = torch.cat([z1, z2], dim=0)  # [2*batch_size, dim]
        
        # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity = torch.matmul(features, features.T)  # [2*batch_size, 2*batch_size]
        
        # 4. åˆ›å»ºæ ‡ç­¾ - ä¿®æ­£ç‰ˆæœ¬
        labels = torch.zeros(2 * batch_size, dtype=torch.long, device=self.device)
        for i in range(batch_size):
            labels[i] = i + batch_size - 1  # ç¬¬ä¸€ä¸ªviewçš„æ­£æ ·æœ¬
            labels[i + batch_size] = i      # ç¬¬äºŒä¸ªviewçš„æ­£æ ·æœ¬
        
        # 5. ç§»é™¤å¯¹è§’çº¿ï¼ˆè‡ªèº«å¯¹æ¯”ï¼‰
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=self.device)
        similarity = similarity[~mask].view(2 * batch_size, -1)
        
        # 6. åº”ç”¨æ¸©åº¦å‚æ•°
        temperature = self.config['model'].get('temperature', 0.5)
        similarity /= temperature
        
        # 7. è®¡ç®—æŸå¤±
        loss = self.criterion(similarity, labels)
        
        loss.backward()
        self.optimizer.step()
        
        return loss.item(), 0.0, batch_size  # SimCLRæ— å‡†ç¡®ç‡æ¦‚å¿µ
    
    def _train_moco_batch(self, batch):
        """MoCoå•æ‰¹æ¬¡è®­ç»ƒ"""
        x1, x2, _ = batch
        x1, x2 = x1.to(self.device), x2.to(self.device)
        
        self.optimizer.zero_grad()
        logits, labels, _, _ = self.model(x1, x2)
        
        loss = self.criterion(logits, labels)
        
        # è®¡ç®—å¯¹æ¯”å‡†ç¡®ç‡
        _, predicted = logits.max(1)
        acc = (predicted == labels).float().mean().item()
        
        loss.backward()
        self.optimizer.step()
        
        return loss.item(), acc, x1.size(0)
    
    def save_checkpoint(self, epoch, loss, acc, path):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss,
            'acc': acc,
            'config': self.config
        }, path)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {path}")
    
    def save_features(self, test_loader, save_path):
        """ä¿å­˜ç‰¹å¾ç”¨äºå¯è§†åŒ–"""
        self.model.eval()
        all_features = []
        all_labels = []
        
        with torch.no_grad():
            for batch in test_loader:
                x1, x2, labels = batch
                x1 = x1.to(self.device)
                
                if self.config['model']['name'] == 'supervised':
                    _, features = self.model(x1)
                else:
                    features = self.model.encode(x1)
                
                all_features.append(features.cpu().numpy())
                all_labels.append(labels.numpy())
        
        all_features = np.concatenate(all_features, axis=0)
        all_labels = np.concatenate(all_labels, axis=0)
        
        np.save(save_path + '_features.npy', all_features)
        np.save(save_path + '_labels.npy', all_labels)
        
        print(f"âœ… ç‰¹å¾å·²ä¿å­˜åˆ°: {save_path}_features.npy")
        print(f"   ç‰¹å¾å½¢çŠ¶: {all_features.shape}, æ ‡ç­¾å½¢çŠ¶: {all_labels.shape}")
