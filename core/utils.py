"""
å·¥å…·å‡½æ•°æ¨¡å—
"""
import os
import random
import numpy as np
import torch
import torch.nn as nn
import yaml
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional, List

def setup_seed(seed: int = 42) -> None:
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ç¡®ä¿ç¡®å®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print(f"âœ… éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")

def get_device(use_cuda: bool = True) -> torch.device:
    """è·å–å¯ç”¨è®¾å¤‡"""
    if use_cuda and torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        device = torch.device('cpu')
        print("âš ï¸  ä½¿ç”¨CPU")
    
    return device

def save_checkpoint(
    state: Dict[str, Any],
    filename: str,
    is_best: bool = False,
    best_filename: str = None
) -> None:
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
    torch.save(state, filename)
    if is_best and best_filename:
        torch.save(state, best_filename)
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_filename}")

def load_checkpoint(
    filename: str, 
    model: nn.Module, 
    optimizer: torch.optim.Optimizer = None,
    device: torch.device = None
) -> Dict[str, Any]:
    """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
    if device is None:
        device = torch.device('cpu')
    
    checkpoint = torch.load(filename, map_location=device)
    
    # åŠ è½½æ¨¡å‹å‚æ•°
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    
    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    print(f"ğŸ“‚ å·²åŠ è½½æ£€æŸ¥ç‚¹: {filename}")
    print(f"   è½®æ¬¡: {checkpoint.get('epoch', 'N/A')}")
    print(f"   æŸå¤±: {checkpoint.get('loss', 'N/A'):.4f}")
    
    return checkpoint

def compute_accuracy(output, target, topk=(1,)):
    """è®¡ç®—top-kå‡†ç¡®ç‡"""
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)
            res.append(correct_k.mul_(100.0 / batch_size))
        return res

class AverageMeter:
    """è®¡ç®—å’Œå­˜å‚¨å¹³å‡å€¼å’Œå½“å‰å€¼"""
    def __init__(self, name: str, fmt: str = ':f'):
        self.name = name
        self.fmt = fmt
        self.reset()
    
    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0
    
    def update(self, val, n: int = 1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count
    
    def __str__(self):
        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
        return fmtstr.format(**self.__dict__)

class ProgressMeter:
    """è¿›åº¦æ˜¾ç¤ºå™¨"""
    def __init__(self, num_batches: int, meters: List[AverageMeter], prefix: str = ""):
        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)
        self.meters = meters
        self.prefix = prefix
    
    def display(self, batch: int):
        entries = [self.prefix + self.batch_fmtstr.format(batch)]
        entries += [str(meter) for meter in self.meters]
        print('\t'.join(entries))
    
    def _get_batch_fmtstr(self, num_batches):
        num_digits = len(str(num_batches // 1))
        fmt = '{:' + str(num_digits) + 'd}'
        return '[' + fmt + '/' + fmt.format(num_batches) + ']'

def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # è®¾ç½®é»˜è®¤å€¼
    defaults = {
        'data': {'num_workers': 2, 'train_split': 0.9, 'val_split': 0.1},
        'training': {'save_frequency': 10, 'save_best_only': True},
        'logging': {'tensorboard': False}
    }
    
    # åˆå¹¶é…ç½®
    for section, default_values in defaults.items():
        if section in config:
            for key, value in default_values.items():
                config[section].setdefault(key, value)
        else:
            config[section] = default_values
    
    print(f"ğŸ“‹ é…ç½®æ–‡ä»¶å·²åŠ è½½: {config_path}")
    return config

def save_config(config: Dict[str, Any], config_path: str) -> None:
    """ä¿å­˜é…ç½®åˆ°YAMLæ–‡ä»¶"""
    os.makedirs(os.path.dirname(config_path), exist_ok=True)
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜: {config_path}")

def save_training_history(history: Dict[str, Any], save_path: str) -> None:
    """ä¿å­˜è®­ç»ƒå†å²"""
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # æ·»åŠ æ—¶é—´æˆ³
    history['timestamp'] = datetime.now().isoformat()
    history['total_time'] = time.time() - history.get('start_time', time.time())
    
    with open(save_path, 'w') as f:
        json.dump(history, f, indent=2, default=str)
    
    print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: {save_path}")

def print_config_summary(config: Dict[str, Any]) -> None:
    """æ‰“å°é…ç½®æ‘˜è¦"""
    print("\n" + "="*50)
    print("ğŸ“‹ é…ç½®æ‘˜è¦")
    print("="*50)
    
    # æ¨¡å‹é…ç½®
    if 'model' in config:
        print(f"ğŸ¤– æ¨¡å‹: {config['model'].get('name', 'æœªçŸ¥')}")
        if config['model'].get('name') == 'simclr':
            print(f"   æŠ•å½±ç»´åº¦: {config['model'].get('projection_dim', 128)}")
            print(f"   æ¸©åº¦å‚æ•°: {config['model'].get('temperature', 0.5)}")
        elif config['model'].get('name') == 'moco':
            print(f"   é˜Ÿåˆ—å¤§å°: {config['model'].get('queue_size', 65536)}")
            print(f"   åŠ¨é‡å‚æ•°: {config['model'].get('momentum', 0.999)}")
    
    # æ•°æ®é…ç½®
    if 'data' in config:
        print(f"ğŸ“Š æ•°æ®: {config['data'].get('name', 'æœªçŸ¥')}")
        print(f"   æ‰¹å¤§å°: {config['data'].get('batch_size', 256)}")
        print(f"   æ•°æ®çº¿ç¨‹: {config['data'].get('num_workers', 2)}")
    
    # è®­ç»ƒé…ç½®
    if 'training' in config:
        print(f"ğŸ‹ï¸  è®­ç»ƒè½®æ¬¡: {config['training'].get('epochs', 100)}")
        print(f"   å­¦ä¹ ç‡: {config['training'].get('learning_rate', 0.001)}")
        print(f"   ä¼˜åŒ–å™¨: {config['training'].get('optimizer', 'adam')}")
    
    print("="*50 + "\n")