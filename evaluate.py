"""è¯„ä¼°è„šæœ¬ - åˆ˜é•¿è’™ä½¿ç”¨"""
import argparse
import os
import json
import torch
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from core.data import get_dataloader
from core.models import create_model

def linear_evaluation(model_path, output_dir):
    """çº¿æ€§è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹"""
    # 1. åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(model_path, map_location='cpu')
    config = checkpoint['config']
    
    # 2. åˆ›å»ºæ¨¡å‹
    model = create_model(config['model']['name'], config['model'])
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # 3. è·å–æ•°æ®
    train_loader = get_dataloader(config, train=True)
    test_loader = get_dataloader(config, train=False)
    
    # 4. æå–ç‰¹å¾
    def extract_features(loader):
        features, labels = [], []
        with torch.no_grad():
            for x1, x2, label in loader:
                if config['model']['name'] == 'supervised':
                    _, feat = model(x1)
                else:
                    feat = model.encode(x1)
                features.append(feat.numpy())
                labels.append(label.numpy())
        return np.concatenate(features), np.concatenate(labels)
    
    print("ğŸ“Š æå–ç‰¹å¾...")
    train_features, train_labels = extract_features(train_loader)
    test_features, test_labels = extract_features(test_loader)
    
    # 5. è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨
    print("ğŸ”§ è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨...")
    clf = LogisticRegression(max_iter=1000, random_state=42)
    clf.fit(train_features, train_labels)
    
    # 6. è¯„ä¼°
    train_pred = clf.predict(train_features)
    test_pred = clf.predict(test_features)
    
    train_acc = accuracy_score(train_labels, train_pred)
    test_acc = accuracy_score(test_labels, test_pred)
    
    # 7. ä¿å­˜ç»“æœ
    results = {
        'model': config['model']['name'],
        'linear_train_accuracy': float(train_acc),
        'linear_test_accuracy': float(test_acc),
        'num_train_samples': len(train_labels),
        'num_test_samples': len(test_labels),
        'checkpoint_epoch': checkpoint['epoch'],
        'checkpoint_accuracy': float(checkpoint.get('acc', 0))
    }
    
    os.makedirs(output_dir, exist_ok=True)
    result_file = os.path.join(output_dir, f"{config['model']['name']}_linear_eval.json")
    
    with open(result_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print("\nğŸ“ˆ çº¿æ€§è¯„ä¼°ç»“æœ:")
    print(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2%}")
    print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2%}")
    print(f"  ç»“æœä¿å­˜åˆ°: {result_file}")
    
    return results

def evaluate_all(models_dir='./results', output_dir='./tables'):
    """è¯„ä¼°æ‰€æœ‰æ¨¡å‹å¹¶ç”Ÿæˆè¡¨æ ¼"""
    os.makedirs(output_dir, exist_ok=True)
    
    models = ['supervised', 'simclr', 'moco']
    all_results = {}
    
    for model_name in models:
        model_path = os.path.join(models_dir, model_name, 'model_best.pt')
        if os.path.exists(model_path):
            print(f"\nğŸ” è¯„ä¼° {model_name}...")
            results = linear_evaluation(model_path, output_dir)
            all_results[model_name] = results
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ° {model_name} æ¨¡å‹")
    
    # ç”Ÿæˆè¡¨æ ¼
    generate_table(all_results, output_dir)
    
    return all_results

def generate_table(results, output_dir):
    """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨æ ¼"""
    import pandas as pd
    
    table_data = []
    for model_name, res in results.items():
        table_data.append({
            'æ–¹æ³•': {'supervised': 'ç›‘ç£å­¦ä¹ ', 'simclr': 'SimCLR', 'moco': 'MoCo v2'}[model_name],
            'çº¿æ€§è¯„ä¼°å‡†ç¡®ç‡ (%)': f"{res['linear_test_accuracy']*100:.1f}",
            'è®­ç»ƒæ ·æœ¬æ•°': res['num_train_samples'],
            'æµ‹è¯•æ ·æœ¬æ•°': res['num_test_samples']
        })
    
    df = pd.DataFrame(table_data)
    
    # ä¿å­˜ä¸ºå¤šç§æ ¼å¼
    df.to_csv(os.path.join(output_dir, 'table1_performance.csv'), index=False)
    df.to_markdown(os.path.join(output_dir, 'table1_performance.md'), index=False)
    
    print("\nğŸ“‹ è¡¨æ ¼å·²ç”Ÿæˆ:")
    print(df.to_string(index=False))
    
    return df

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='è¯„ä¼°æ¨¡å‹')
    parser.add_argument('--model', type=str, choices=['supervised', 'simclr', 'moco', 'all'],
                       default='all', help='è¦è¯„ä¼°çš„æ¨¡å‹')
    parser.add_argument('--input', type=str, default='./results',
                       help='æ¨¡å‹ç›®å½•')
    parser.add_argument('--output', type=str, default='./tables',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    if args.model == 'all':
        evaluate_all(args.input, args.output)
    else:
        model_path = os.path.join(args.input, args.model, 'model_best.pt')
        linear_evaluation(model_path, args.output)